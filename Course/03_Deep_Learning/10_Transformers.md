# **Transformers** â€“ modelo dominante en NLP y otras Ã¡reas.

## ğŸš€ Â¿QuÃ© es un Transformer?

Es una arquitectura que reemplaza las RNN y LSTM para manejar secuencias, pero sin procesar paso a paso. En vez de eso, **procesa toda la secuencia de una vez**, usando un mecanismo llamado **AtenciÃ³n**.

---

## ğŸ” Â¿Por quÃ© es tan poderoso?

- Puede capturar **relaciones a largo plazo** en la secuencia sin perder contexto.
- Es **paralelizable**, asÃ­ que entrena mucho mÃ¡s rÃ¡pido que RNN/LSTM.
- Es la base de modelos como GPT, BERT, y otros gigantes del lenguaje.

---

## âš™ï¸ Conceptos clave:

- **Self-Attention:** Cada palabra â€œpresta atenciÃ³nâ€ a todas las otras palabras de la secuencia para entender contexto.
- **Positional Encoding:** Como no procesa secuencia en orden, usa esta tÃ©cnica para saber el orden de las palabras.
- **Capas y bloques:** Construidos con multi-head attention y feed-forward layers.

---

## ğŸ¯ AnalogÃ­a simple:

ImaginÃ¡ que estÃ¡s leyendo un pÃ¡rrafo y podÃ©s instantÃ¡neamente ver cÃ³mo cada palabra se relaciona con todas las demÃ¡s, sin tener que leer en orden uno a uno.

### ğŸ” Paso 1: Â¿QuÃ© es la atenciÃ³n (Attention)?

La atenciÃ³n es como un **radar inteligente** que, para cada palabra de la secuencia, decide a quÃ© otras palabras debe â€œprestar atenciÃ³nâ€ para entender mejor su significado.

---

### ğŸ§  Ejemplo simple:

Frase:

*â€œEl banco cerrÃ³ temprano porque estaba lloviendo.â€*

La palabra â€œbancoâ€ puede significar â€œinstituciÃ³n financieraâ€ o â€œbanco para sentarseâ€.

Para saber cuÃ¡l es, la atenciÃ³n hace que â€œbancoâ€ mire palabras como â€œcerrÃ³â€ y â€œlloviendoâ€ para entender el contexto.

---

### ğŸ”§ Paso 2: Â¿CÃ³mo se calcula la atenciÃ³n?

Para cada palabra (o â€œtokenâ€) se crean tres vectores:

- **Query (Q):** La palabra que estÃ¡ buscando informaciÃ³n.
- **Key (K):** Las palabras contra las que se compara.
- **Value (V):** La informaciÃ³n que se recupera.

---

### ğŸ” Paso 3: El proceso

1. Se calcula la similitud (un producto punto) entre el Query de la palabra actual y los Keys de todas las palabras.
2. Se normalizan esas similitudes con softmax para obtener pesos (quÃ© tanto â€œatenderâ€ a cada palabra).
3. Se usa esos pesos para hacer una combinaciÃ³n ponderada de los Values.
4. Esa combinaciÃ³n es la **atenciÃ³n final** para la palabra actual.

---

### ğŸ”¥ Â¿Por quÃ© es poderoso?

- Porque cada palabra **puede considerar todo el contexto**, no solo las palabras cercanas.
- Esto soluciona problemas de largo alcance que las RNN/LSTM tenÃ­an.

---

### ğŸ“¦ Bonus: Multi-head attention

El Transformer no hace esto una sola vez, sino varias veces en paralelo (con diferentes parÃ¡metros), para capturar distintos tipos de relaciones y patrones.