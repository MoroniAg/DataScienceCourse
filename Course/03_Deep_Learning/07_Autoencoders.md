# **Autoencoders** â€“ compresiÃ³n y reconstrucciÃ³n.

## ğŸ§  Â¿QuÃ© es un Autoencoder?

Es un tipo de red neuronal **no supervisada** que aprende a **comprimir datos y luego reconstruirlos**.

ğŸ‘‰ Â¿Te suena a zip-descomprimir? Exactamente.

---

### ğŸ“¦ Estructura bÃ¡sica:

Un Autoencoder tiene 3 partes:

1. **Encoder (codificador)**
    - Toma el input y lo **comprime** en una versiÃ³n mÃ¡s pequeÃ±a (llamada *cÃ³digo* o *embedding*).
2. **Latent Space (espacio latente)**
    - El â€œresumenâ€ de los datos originales.
    - Es lo que la red cree que es **lo mÃ¡s importante** del input.
3. **Decoder (decodificador)**
    - Toma el cÃ³digo comprimido y trata de **reconstruir el input original** lo mejor posible.

---

## ğŸ¨ AnalogÃ­a visual

ImaginÃ¡ que te dan una imagen de 256x256 pÃ­xeles y te dicen:

> â€œNo la guardes entera. Dibujala tÃº mismo desde la memoria, pero solo podÃ©s usar 50 nÃºmeros.â€
> 

El encoder serÃ­a tu capacidad de **resumir** lo esencial.

El decoder serÃ­a tu capacidad de **recrear la imagen con lo poco que te acordÃ¡s**.

---

### ğŸ§© Â¿Para quÃ© sirven los Autoencoders?

1. **ReducciÃ³n de dimensionalidad**
    
    (como alternativa a PCA pero no lineal)
    
2. **DetecciÃ³n de anomalÃ­as**
    
    Si la red no puede reconstruir bien un dato â†’ probablemente sea un dato raro.
    
3. **EliminaciÃ³n de ruido (Denoising Autoencoders)**
    
    EntrenÃ¡s a la red con inputs ruidosos y salidas limpias.
    
4. **Preentrenamiento de redes profundas**
    
    Puedes usar el encoder como base para otras tareas.
    
5. **GeneraciÃ³n de datos**
    
    Como paso previo a modelos generativos mÃ¡s complejos (VAE, GANs).
    

---

### ğŸ“Œ Â¿Con quÃ© se entrenan?

Con una **pÃ©rdida de reconstrucciÃ³n**, por ejemplo:

> Â¿QuÃ© tan diferente es la salida comparada con el input original?
> 

ğŸ” Se entrena para que esa diferencia sea mÃ­nima.