# **Long Short-Term Memory (LSTM)** â€“ mejora de RNNs.

## ğŸ˜µâ€ğŸ’« EL PROBLEMA REAL DE LAS RNN:

### ğŸ‘‰ **Â¡Se les olvida todo a largo plazo!**

Si le das una frase muy larga como:

> â€œHace 20 aÃ±os, en un pequeÃ±o pueblo donde la vida era tranquila, naciÃ³ un niÃ±o llamado EfraÃ­n, que un dÃ­aâ€¦â€
> 

Cuando llegÃ¡s al final, la RNN ya se **olvidÃ³ de lo del principio**, porque la â€œmemoriaâ€ se va diluyendo. Eso se llama:

### ğŸ”» *Desvanecimiento del gradiente*

---

## ğŸ§  Â¿QuÃ© es una LSTM entonces?

**LSTM = Long Short-Term Memory**

â†’ Es una evoluciÃ³n de la RNN que tiene **una memoria mÃ¡s inteligente**

â†’ Decide **quÃ© recordar**, **quÃ© olvidar** y **cuÃ¡ndo usar lo que recuerda**

SÃ­, es como una RNN, pero con **agenda, criterio y sentido comÃºn.**

---

## ğŸ§­ ANALOGÃA PRÃCTICA

ImaginÃ¡ que sos un estudiante y tenÃ©s que leer un libro de 300 pÃ¡ginas para un examen.

- Una **RNN** lee todo pero no anota nada. Va olvidando lo anterior mientras avanza.
- Una **LSTM**, en cambio, **tiene un cuaderno**:
    - âœï¸ Apunta lo importante
    - âŒ Borra lo irrelevante
    - âœ… Usa sus apuntes cuando los necesita

---

## ğŸ”§ MECANISMO INTERNO DE LA LSTM

Una LSTM tiene **"puertas"**. Y no, no es Hogwarts, aunque lo parece:

### ğŸšª1. *Forget Gate* (puerta de olvido)

Decide **quÃ© parte de la memoria anterior descartar**

â†’ "Esto ya no es relevante, bÃ³rralo."

### ğŸšª2. *Input Gate* (puerta de entrada)

Decide **quÃ© nueva informaciÃ³n guardar**

â†’ "Esto es importante, anÃ³talo."

### ğŸšª3. *Output Gate* (puerta de salida)

Decide **quÃ© parte de la memoria usar ahora mismo**

â†’ "Necesito recordar esto para la salida actual."

---

## ğŸ” FUNCIONAMIENTO EN PASOS

En cada timestep (paso en la secuencia):

1. Mira la entrada actual
2. Mira lo que tiene en su memoria
3. Usa las puertas para:
    - Limpiar lo inÃºtil
    - Guardar lo Ãºtil
    - Decidir quÃ© mostrar como salida

Resultado: **memoria mÃ¡s estable, mÃ¡s precisa y mÃ¡s duradera**

---

## ğŸ“š Â¿CuÃ¡ndo usar una LSTM?

- Texto largo con dependencias distantes (novelas, artÃ­culos)
- Series de tiempo financieras
- PredicciÃ³n meteorolÃ³gica
- Audio y mÃºsica con estructuras complejas
- GeneraciÃ³n de texto (como lo hace GPT pero a nivel simple)

---

## ğŸ¤¯ Miniresumen

| CaracterÃ­stica | RNN | LSTM |
| --- | --- | --- |
| Memoria | Corto plazo | Largo y corto plazo |
| Â¿Olvida con el tiempo? | SÃ­ | Mucho menos |
| Ideal para | Secuencias cortas | Secuencias largas y complejas |

## ğŸ¯ PARTE 1: EJEMPLO COMPARATIVO RNN vs LSTM

ImaginÃ¡ que querÃ©s predecir la **palabra final** de una frase larga.

Frase:

> â€œEl niÃ±o que vivÃ­a en la casa azul, junto a su perro, su bicicleta, sus juguetes, su vecina, y su abuela favorita... siempre fue muy __________.â€
> 

---

### ğŸ§  Â¿QuÃ© hace una RNN?

- Empieza bien, entendiendo â€œniÃ±oâ€, â€œcasaâ€, â€œperroâ€...
- Pero a medida que avanza, empieza a **olvidar el sujeto original**.
- Cuando llega al final, ya no recuerda que hablÃ¡bamos del niÃ±o.
- Resultado: puede decir â€œtranquilaâ€, â€œruidosaâ€, â€œsolitariaâ€ â†’ palabras sin contexto correcto.

---

### ğŸ§  Â¿QuÃ© hace una LSTM?

- Usa su â€œcuaderno mentalâ€ (las **puertas**) para **recordar el sujeto: el niÃ±o**.
- Aunque aparezcan muchos detalles intermedios, **no lo pierde de vista**.
- Llega al final y dice: *â€œFue muy felizâ€*, *â€œfue muy curiosoâ€* â†’ respuestas que **tienen sentido con el principio**.

ğŸ§  **LSTM = memoria selectiva + contexto largo**

---

## ğŸ” PARTE 2: Â¿CÃ“MO FUNCIONAN LAS PUERTAS DE UNA LSTM?

Vamos a visualizarlo como una **lÃ­nea de producciÃ³n en una fÃ¡brica de recuerdos**, con 3 controles de paso.

---

### ğŸ’¡ Cada paso de la secuencia pasa por 3 â€œpuertasâ€:

### 1. ğŸ§½ **Forget Gate**

> â€œÂ¿Hay algo de lo anterior que deba tirar?â€
> 
- Si estÃ¡s leyendo â€œHoy comÃ­ arroz, luego fui al parque...â€,
    
    al llegar al parque, capaz el arroz **ya no importa** â†’ lo olvida.
    

### 2. âœï¸ **Input Gate**

> â€œÂ¿Esta nueva informaciÃ³n vale la pena guardarla?â€
> 
- Si leÃ©s â€œ...y vi a mi mejor amigo despuÃ©s de aÃ±osâ€,
    
    eso **sÃ­ se guarda**, es relevante.
    

### 3. ğŸ“¤ **Output Gate**

> â€œÂ¿QuÃ© parte de lo que tengo en la memoria quiero usar ahora mismo?â€
> 
- QuizÃ¡ recordÃ¡s que **te hizo reÃ­r mucho**, y eso **influye en tu respuesta emocional actual**.

---

### âš™ï¸ Diagrama mental simple:

```
           Entrada actual
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Forget gate         â”‚  â† borra lo innecesario
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Input gate          â”‚  â† guarda lo Ãºtil
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Output gate         â”‚  â† usa lo relevante ahora
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
              Salida

```

ğŸ” Todo eso se repite en **cada paso de la secuencia**.

Y todo se entrena con backpropagation como en una red normal, pero cada puerta tiene **sus propios pesos y decisiones**.