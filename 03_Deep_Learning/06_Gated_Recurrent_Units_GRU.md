# **Gated Recurrent Units (GRU)** â€“ otra variante de RNN.

## ğŸ§  Â¿QuÃ© es un GRU?

**GRU = Gated Recurrent Unit**

Es una versiÃ³n mÃ¡s simple (y mÃ¡s rÃ¡pida) de la LSTM, pero que aÃºn **mantiene buena memoria** para secuencias.

---

### ğŸ“Œ ComparaciÃ³n rÃ¡pida:

| CaracterÃ­stica | RNN | LSTM | GRU |
| --- | --- | --- | --- |
| Â¿Tiene memoria? | SÃ­ | SÃ­, con 3 puertas | SÃ­, con 2 puertas |
| Â¿Es compleja? | Baja | Alta | Media |
| Â¿Es precisa? | Media | Alta (en secuencias largas) | Casi igual a LSTM |
| Â¿Es mÃ¡s rÃ¡pida? | SÃ­ | âŒ No | âœ… SÃ­ |

---

## ğŸ” Â¿QuÃ© hace un GRU diferente?

Mientras que la **LSTM tiene 3 puertas** (Forget, Input, Output), la **GRU combina y reduce**:

### ğŸ”‘ GRU solo tiene 2 puertas:

### 1. **Update Gate (puerta de actualizaciÃ³n)**

> Decide cuÃ¡nto de la memoria anterior se mantiene y cuÃ¡nto se reemplaza.
> 

### 2. **Reset Gate (puerta de reinicio)**

> Decide cuÃ¡nto olvidar del pasado para procesar la nueva entrada.
> 

---

### ğŸ§  Analogia sencilla:

ImaginÃ¡ que estÃ¡s tomando notas mientras alguien habla rÃ¡pido.

- Si usÃ¡s una **LSTM**, tenÃ©s 3 bolÃ­grafos de colores para:
    - Subrayar lo importante
    - Tachar lo que no sirve
    - Escribir lo nuevo
        
        â†’ Preciso, pero lento.
        
- Si usÃ¡s una **GRU**, tenÃ©s solo 2 resaltadores:
    - Uno para lo que vale la pena mantener
    - Otro para decidir si ignorÃ¡s lo anterior
        
        â†’ MÃ¡s rÃ¡pido, menos sobrecargado.
        

---

### ğŸ“¦ Â¿CuÃ¡ndo usar GRU?

âœ… Cuando:

- QuerÃ©s buena performance **con menos cÃ³mputo**
- EstÃ¡s trabajando con secuencias **no tan largas**
- NecesitÃ¡s entrenar **rÃ¡pido** sin perder precisiÃ³n

---

### â—Â¿Pierde memoria como una RNN?

No.

Aunque es mÃ¡s simple que LSTM, el GRU sigue teniendo **una memoria sÃ³lida**, solo que con una estructura mÃ¡s compacta.